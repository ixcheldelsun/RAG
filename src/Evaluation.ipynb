{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run RAG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions_path = rags_for_eval[0].get_evaluation_questions()\n",
    "eval_questions = []\n",
    "with open(eval_questions_path, 'r') as file:\n",
    "    for line in file:\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.custom import instrument\n",
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trulens_app(rag, feedbacks:list):\n",
    "  \"\"\"\n",
    "  Get trulens app for a given rag and feedbacks.\n",
    "\n",
    "  Args:\n",
    "    rag: RAG object.\n",
    "    feedbacks: List of feedbacks.\n",
    "\n",
    "  Returns:\n",
    "    trulens app.\n",
    "  \"\"\"\n",
    "  from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "  return TruCustomApp(\n",
    "    rag,\n",
    "    app_name=\"RAG\",\n",
    "    app_version=rag.name,\n",
    "    feedbacks=feedbacks,\n",
    "  )\n",
    "  \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def build_trulens_recorder(rag):\n",
    "  \"\"\"\n",
    "  Build trulens recorder for a given rag, setting the feedbacks functions and building the recorder app.\n",
    "\n",
    "  Args:\n",
    "    rag: RAG object.\n",
    "\n",
    "  Returns:\n",
    "    trulens recorder app.\n",
    "  \"\"\"\n",
    "  from trulens.providers.openai import OpenAI\n",
    "  from trulens.core import Feedback\n",
    "  from trulens.core import Select\n",
    "  import numpy as np\n",
    "\n",
    "  provider = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "  # Define a groundedness feedback function\n",
    "  f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .on_output()\n",
    "  )\n",
    "\n",
    "  # Question/answer relevance between overall question and answer.\n",
    "  f_answer_relevance = (\n",
    "    Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    "  )\n",
    "\n",
    "  # Question/statement relevance between question and each context chunk.\n",
    "  f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.retrieve.rets[:])\n",
    "    .aggregate(np.mean)\n",
    "  )\n",
    "  \n",
    "\n",
    "  feedbacks = [f_groundedness, f_answer_relevance, f_context_relevance]\n",
    "\n",
    "  return get_trulens_app(rag, feedbacks)\n",
    "\n",
    "\n",
    "def eval_rags(rags:list, questions:list):\n",
    "  \"\"\"\n",
    "  Evaluate a list of rags for a list of questions.\n",
    "\n",
    "  Args:\n",
    "    rags: List of RAG objects.\n",
    "    questions: List of questions for evaluation.\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  from tqdm import tqdm\n",
    "\n",
    "  for r in rags:\n",
    "    print(\"Evaluating: \", r.name)\n",
    "    tru_query_engine_recorder = build_trulens_recorder(r)\n",
    "\n",
    "    with tru_query_engine_recorder as recording:\n",
    "      for q in tqdm(questions):\n",
    "        r.query(q)\n",
    "\n",
    "    print(\"\\nFinished evaluation\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rags(rags_for_eval, eval_questions[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(\n",
    "    session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens.dashboard import stop_dashboard\n",
    "\n",
    "# stop_dashboard(\n",
    "#     session\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".eval_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
