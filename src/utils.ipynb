{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse documents from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Bucket.ipynb\n",
    "%run Firebase.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import settings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# Importante saber a detalle que hace SmartPDFLoader para la presentaciÃ³n\n",
    "from llama_index.readers.smart_pdf_loader import SmartPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_INPUT_BUCKET = settings.aws_s3_input_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = Bucket(AWS_S3_INPUT_BUCKET)\n",
    "\n",
    "# Parsing service\n",
    "ec2_instance_ip = \"3.18.101.52\"\n",
    "llmsherpa_api_url = f\"http://{ec2_instance_ip}/api/parseDocument?renderFormat=all\"\n",
    "pdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n",
    "\n",
    "# Define a local directory to temporarily store downloaded files\n",
    "local_directory = './s3_files/'\n",
    "\n",
    "# List objects in the S3 bucket\n",
    "response = bucket.list_objects()\n",
    "documents = {}\n",
    "\n",
    "print(f\"Found {len(response)} objects\")\n",
    "document_titles = [obj['Key'] for obj in response]\n",
    "print(f\"The documents in bucket are the following:\", document_titles)\n",
    "\n",
    "processed_document_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmsherpa_api_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the bucket has any files\n",
    "if len(response) > 0:\n",
    "    for obj in tqdm(document_titles):\n",
    "        # Download the file from S3\n",
    "        pdf_file_path = bucket.download_object(obj, local_directory, return_file_path=True)\n",
    "\n",
    "        # Process the file with SmartPDF\n",
    "        print(f\"Loading {obj} with SmartPDF...\")\n",
    "        document = pdf_loader.load_data(pdf_file_path)\n",
    "        \n",
    "        print(f\"Loaded {obj} with SmartPDF.\")\n",
    "        # Upload document HERE and create a dict that identifies each llama parsed document with its origin PDF\n",
    "        documents[obj] = document\n",
    "        processed_document_titles.append(obj)\n",
    "        time.sleep(2)\n",
    "else:\n",
    "    print(\"No files found in the S3 bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add doc title to metadata from documents dictionary\n",
    "for key, docs in documents.items():\n",
    "  list(\n",
    "      map(\n",
    "          lambda doc: doc.metadata.update({'title': key}),\n",
    "          docs\n",
    "          )\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_documents = [item for sublist in list(documents.values()) for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "\n",
    "pending_docs = []\n",
    "with open('parsed_docs/docs.txt', 'r') as file:\n",
    "    line = file.readline()\n",
    "    while line:\n",
    "        pending_docs.append(eval(line))\n",
    "        line = file.readline()\n",
    "\n",
    "pending_docs = [Document(**doc) for doc in pending_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firebase = Firebase()\n",
    "stored_docs = firebase.get_all_document_ids()\n",
    "docs_to_store = [doc for doc in pending_docs if doc.id_ not in stored_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firebase.upload_documents(docs_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('parsed_docs', exist_ok=True)\n",
    "with open('parsed_docs/docs.txt', 'w') as file:\n",
    "    for d in docs_to_store:\n",
    "        file.write(f\"{d.__dict__}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Evaluation questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get from ChatGPT questions for given input documents\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class QuestionGenerator:\n",
    "\n",
    "  def __init__(self):\n",
    "    print(\"Initiating Question Generator with OpenAI\")\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "  def generate_question_from_document(self, llama_document) -> str:\n",
    "      # Step 1: Get the content from the LlamaIndex document\n",
    "      document_content = llama_document.get_text()\n",
    "      print(\"Generating a question for the following text:\", document_content)\n",
    "\n",
    "      # Step 2: Prepare the prompt to ask GPT to generate a question\n",
    "      system = \"You are a helpful assistant that generates questions in Spanish given some texts. The questions must: \\n1. Not repeat.\\n2. Be only about something in the text.\\n3. If not question can be made from the text, return an empty value like ''.\"\n",
    "      messages = [{\"role\": \"system\", \"content\": system},]\n",
    "\n",
    "      prompt = f\"Read the following document and generate a relevant question about its content in Spanish:\\n\\n{document_content}\"\n",
    "      messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "      # Step 3: Use OpenAI to generate the question\n",
    "      client = openai.OpenAI()\n",
    "\n",
    "      response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "      )\n",
    "\n",
    "      return response.choices[0].message.content\n",
    "\n",
    "  def generate_questions_from_documents(self, llama_documents) -> list:\n",
    "    questions = []\n",
    "    for llama_document in tqdm(llama_documents):\n",
    "      question = self.generate_question_from_document(llama_document)\n",
    "      questions.append(question)\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read s3_files directory and get all the files\n",
    "files_detected = os.listdir('s3_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_eval = {}\n",
    "firebase = Firebase()\n",
    "# Group documents by file it belongs to\n",
    "for file in files_detected:\n",
    "  print(\"Building documents and questions for: \", file)\n",
    "  documents_to_eval[file] = {}\n",
    "  documents_to_eval[file]['documents'] = firebase.get_all_documents(limit=50, document_title=file)\n",
    "  documents_to_eval[file]['questions'] =  question_generator.generate_questions_from_documents(documents_to_eval[file]['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, vals in documents_to_eval.items():\n",
    "  print(\"File: \", file)\n",
    "  print(\"Docs: \", vals['documents'])\n",
    "  print(\"Questions: \", vals['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('evaluation', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get only top 5 questions from each document\n",
    "for file, d in documents_to_eval.items():\n",
    "    count = Counter(d['questions'])\n",
    "    top_questions = count.most_common(5)\n",
    "    d['top_questions'] = [q[0] for q in top_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "get_values = lambda data: list(itertools.chain.from_iterable(d['top_questions'] for d in documents_to_eval.values()))\n",
    "\n",
    "questions = get_values(documents_to_eval)\n",
    "\n",
    "questions = list(set(questions))\n",
    "\n",
    "print(\"Number of questions: \", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('evaluation/eval_questions.txt', 'w') as file:\n",
    "    for q in questions:\n",
    "        file.write(f\"{q}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = Bucket('rag-outputs-pdf')\n",
    "bucket.upload_object('evaluation/eval_questions.txt', 'evaluation_questions.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
