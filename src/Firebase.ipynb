{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import settings\n",
    "from llama_index.core import Document\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRESTORE\n",
    "CREDENTIALS_JSON_NAME = settings.credentials_json_name\n",
    "FIRESTORE_COLLECTION = settings.firestore_collection\n",
    "FIRESTORE_PROJECT = settings.firestore_project\n",
    "FIRESTORE_DATABASE = settings.firestore_database\n",
    "\n",
    "# CHUNK\n",
    "CHUNK_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Bucket.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Firebase:\n",
    "\n",
    "  def __init__(self):\n",
    "    print(\"Initialized connection to Firebase\")\n",
    "    from google.oauth2 import service_account\n",
    "    from llama_index.storage.kvstore.firestore import FirestoreKVStore\n",
    "    from llama_index.storage.docstore.firestore import FirestoreDocumentStore\n",
    "\n",
    "    # Initialize Firebase\n",
    "    firebase_credentials_file_path = self._get_credentials()\n",
    "    self.cred = credentials.Certificate(firebase_credentials_file_path)\n",
    "    self.cred_google_auth = service_account.Credentials.from_service_account_file(\n",
    "      firebase_credentials_file_path\n",
    "    )\n",
    "\n",
    "    try:\n",
    "      firebase_admin.initialize_app(self.cred)\n",
    "    except ValueError:\n",
    "      print(\"Firebase app already initialized\")\n",
    "\n",
    "    # Get Firestore client\n",
    "    self.db = firestore.client()\n",
    "    self.collection_name_base = FIRESTORE_COLLECTION\n",
    "    self.collection_name = self.collection_name_base + '_data'\n",
    "    self.project = FIRESTORE_PROJECT\n",
    "    self.database = FIRESTORE_DATABASE\n",
    "    self.chunk_size = CHUNK_SIZE\n",
    "    \n",
    "    self.kvstore = FirestoreKVStore(\n",
    "      project=self.project,\n",
    "      database=self.database,\n",
    "      credentials=self.cred_google_auth\n",
    "    )\n",
    "\n",
    "    self.docstore = FirestoreDocumentStore(\n",
    "        firestore_kvstore=self.kvstore,\n",
    "        namespace=self.collection_name_base\n",
    "    )\n",
    "\n",
    "    print(f\"Set up connection to Firebase Database {self.database} in project {self.project}\")\n",
    "\n",
    "\n",
    "  def _get_credentials(self):\n",
    "    credentials_json = CREDENTIALS_JSON_NAME\n",
    "    bucket = Bucket(AWS_S3_OUTPUT_BUCKET)\n",
    "    firebase_credentials_file_path = bucket.download_object(credentials_json, \"credentials\", return_file_path=True)\n",
    "    return firebase_credentials_file_path\n",
    "\n",
    "  def upload_documents(self, documents):\n",
    "\n",
    "    def chunks(lst):\n",
    "      \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "      for i in range(0, len(lst), self.chunk_size):\n",
    "          yield lst[i:i + self.chunk_size]\n",
    "\n",
    "    print(f\"Uploading {len(documents)} documents to Firebase\")\n",
    "    doc_chunks = chunks(documents)\n",
    "\n",
    "    for chunk in tqdm(doc_chunks):\n",
    "      self._upload_documents(chunk)\n",
    "\n",
    "    print(\"Documents uploaded to Firebase!!\")\n",
    "\n",
    "\n",
    "  def _upload_documents(self, documents):\n",
    "    from llama_index.core.node_parser import SentenceSplitter\n",
    "    \n",
    "    nodes = SentenceSplitter().get_nodes_from_documents(documents)\n",
    "    print(f\"Uploading {len(nodes)} nodes to Firebase...\")\n",
    "    self.docstore.add_documents(nodes)\n",
    "    print(\"Nodes loaded!!\")\n",
    "\n",
    "\n",
    "  def get_document(self, document_id):\n",
    "      print(f\"Getting document {document_id} from collection {self.collection_name}\")\n",
    "      doc_ref = self.db.collection(self.collection_name).document(document_id)\n",
    "      doc = doc_ref.get()\n",
    "      if doc.exists:\n",
    "          print(f\"Document {document_id} found in collection {self.collection_name}\")\n",
    "          return doc.to_dict()\n",
    "      else:\n",
    "          print(f\"Document {document_id} NOT found in collection {self.collection_name}\")\n",
    "          return None\n",
    "\n",
    "\n",
    "  def get_all_documents(self, limit:int = None, only_paragraph:bool = False, document_title:str = None) -> list:\n",
    "      print(f\"Getting all documents from collection {self.collection_name}\")\n",
    "      if limit:\n",
    "        print(f\"Limiting to {limit} documents\")\n",
    "\n",
    "      docs = self.db.collection(self.collection_name)\n",
    "\n",
    "      if only_paragraph:\n",
    "        print(\"Getting only paragraphs in provided documents\")\n",
    "        docs = docs.where('data.metadata.chunk_type', '==', 'para')\n",
    "\n",
    "      if document_title:\n",
    "        print(\"Getting documents from the following file: \", document_title)\n",
    "        docs = docs.where('data.metadata.title', '==', document_title)\n",
    "\n",
    "      if limit:\n",
    "        docs = docs.limit(limit)\n",
    "\n",
    "      docs = docs.stream()\n",
    "\n",
    "      documents = [doc.to_dict() for doc in docs]\n",
    "      return self._documents_to_llama_index_documents(documents)\n",
    "\n",
    "  def get_all_document_ids(self):\n",
    "    # Reference to the collection\n",
    "    docs = self.db.collection(self.collection_name).stream()\n",
    "\n",
    "    # List to store document IDs\n",
    "    document_ids = [doc.id for doc in docs]\n",
    "\n",
    "    return document_ids\n",
    "\n",
    "\n",
    "\n",
    "  def _dict_to_llama_index_document(self, doc_dict):\n",
    "      # Extract fields from the dictionary\n",
    "      document_data = doc_dict.get('data', {})\n",
    "      document_id = document_data.get('id_', None)\n",
    "\n",
    "      if document_id is None:\n",
    "          raise ValueError(\"Document ID not found in the dictionary.\")\n",
    "\n",
    "      text_content = document_data.get('text', '')\n",
    "      metadata = document_data.get('metadata', {})\n",
    "\n",
    "      # Create a LlamaIndex Document\n",
    "      document = Document(\n",
    "          doc_id=document_id,\n",
    "          text=text_content,\n",
    "          metadata=metadata\n",
    "      )\n",
    "\n",
    "      return document\n",
    "\n",
    "  def _documents_to_llama_index_documents(self, documents):\n",
    "      print(\"Converting documents to LlamaIndex documents...\")\n",
    "      import os\n",
    "      from tqdm import tqdm\n",
    "      llama_index_documents = []\n",
    "      for doc in tqdm(documents):\n",
    "          llama_index_document = self._dict_to_llama_index_document(doc)\n",
    "          llama_index_documents.append(llama_index_document)\n",
    "      print(\"Conversion successful!\")\n",
    "      return llama_index_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firebase = Firebase()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
